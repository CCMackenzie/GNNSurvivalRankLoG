{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If running on Google Colab uncomment here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "!pip install -U numpy\n",
    "!pip install umap-learn ujson\n",
    "!pip uninstall -y torch-scatter torch-sparse torch-geometric\n",
    "!pip uninstall -y torch\n",
    "!pip install torch==1.10.0+cu113 torchvision==0.11.1+cu113 torchaudio==0.10.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n",
    "!pip install torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
    "!pip install torch-sparse -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
    "!pip install torch-geometric \n",
    "!pip install lifelines\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "from torch.utils.data import Dataset, Sampler\n",
    "import torch.utils.data as data_utils\n",
    "from torchvision import datasets, transforms\n",
    "from copy import deepcopy\n",
    "from numpy.random import randn \n",
    "from torch.nn import BatchNorm1d\n",
    "from torch.nn import Sequential, Linear, ReLU,Tanh,LeakyReLU,ELU,SELU,GELU\n",
    "from torch_geometric.nn import GINConv,EdgeConv, PNAConv,DynamicEdgeConv,global_add_pool, global_mean_pool, global_max_pool\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from scipy.spatial import distance_matrix, Delaunay\n",
    "import random\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "import pickle\n",
    "from glob import glob\n",
    "import os\n",
    "from sklearn.neighbors import kneighbors_graph, radius_neighbors_graph\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pdb\n",
    "from statistics import mean, stdev\n",
    "from glob import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from torch.autograd import Variable\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.neighbors import kneighbors_graph, radius_neighbors_graph\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "import math\n",
    "from random import shuffle\n",
    "from itertools import islice\n",
    "from lifelines.utils import concordance_index as cindex\n",
    "from lifelines import KaplanMeierFitter\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from collections import OrderedDict\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.00002\n",
    "WEIGHT_DECAY = 0.005\n",
    "L1_WEIGHT = 0.001\n",
    "SCHEDULER = None\n",
    "BATCH_SIZE = 10\n",
    "NUM_BATCHES = 2000\n",
    "NUM_LOGS = 150 # How many times in training the loss value is stored\n",
    "\n",
    "#Select what feature set to use\n",
    "SHUFFLE_NET = True\n",
    "\n",
    "VALIDATION = True\n",
    "NORMALIZE = False\n",
    "CENSORING = True\n",
    "FRAC_TRAIN = 0.8\n",
    "CONCORD_TRACK = True\n",
    "FILTER_TRIPLE = False\n",
    "EARLY_STOPPING = True\n",
    "MODEL_PATH = 'Best_model/'\n",
    "VARIABLES = 'DSS'\n",
    "TIME_VAR = VARIABLES + '.time'\n",
    "ON_GPU = True\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "rng = np.random.default_rng()\n",
    "device = {True:'cuda:0',False:'cpu'}[USE_CUDA] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessory methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cuda(v):\n",
    "    if USE_CUDA:\n",
    "        return v.cuda()\n",
    "    return v\n",
    "\n",
    "def toTensor(v,dtype = torch.float,requires_grad = True):\n",
    "    return torch.from_numpy(np.array(v)).type(dtype).requires_grad_(requires_grad)\n",
    "\n",
    "def toTensorGPU(v,dtype = torch.float,requires_grad = True):\n",
    "    return cuda(torch.from_numpy(np.array(v)).type(dtype).requires_grad_(requires_grad))\n",
    "\n",
    "def toNumpy(v):\n",
    "    if type(v) is not torch.Tensor: return np.asarray(v)\n",
    "    if USE_CUDA:\n",
    "        return v.detach().cpu().numpy()\n",
    "    return v.detach().numpy()\n",
    "\n",
    "def pickleLoad(ifile):\n",
    "    with open(ifile, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "def toGeometric(Gb,y,tt=1e-3):\n",
    "    return Data(x=Gb.x, edge_index=(Gb.get(W)>tt).nonzero().t().contiguous(),y=y)\n",
    "\n",
    "def toGeometricWW(X,W,y,tt=0):    \n",
    "    return Data(x=toTensor(X,requires_grad = False), edge_index=(toTensor(W,requires_grad = False)>tt).nonzero().t().contiguous(),y=toTensor([y],dtype=torch.long,requires_grad = False))\n",
    "\n",
    "\n",
    "def pair_find(graphs,features):\n",
    "    indexes = []\n",
    "    for j in range(len(graphs)):\n",
    "        graph_j = graphs[j]\n",
    "        if features == 'BRCA-SHUFFLE':\n",
    "            event_j = graph_j[1][0]\n",
    "            time_j = graph_j[1][1]\n",
    "        else:\n",
    "            event_j, time_j = graph_j.event, graph_j.e_time\n",
    "        if event_j == 1:\n",
    "            for i in range(len(graphs)): \n",
    "                graph_i = graphs[i]            \n",
    "                if features == 'BRCA-SHUFFLE':\n",
    "                    time_i = graph_i[1][1]\n",
    "                else:\n",
    "                    time_i = graph_i.e_time\n",
    "                if graph_j != graph_i and time_i > time_j:\n",
    "                    indexes.append((i,j))\n",
    "    shuffle(indexes)\n",
    "    return indexes\n",
    "\n",
    "def SplitBrcaData(dataset, numSplits, isShuffle, testSize):\n",
    "    if isShuffle:\n",
    "        eventVars = [dataset[i][1][0] for i in range(len(dataset))]\n",
    "    else:\n",
    "        eventVars = [int(dataset[i].event.detach().numpy()) for i in range(len(dataset))]  \n",
    "    x = np.zeros(len(dataset))\n",
    "    shuffleSplit = StratifiedShuffleSplit(n_splits = numSplits, test_size = testSize)\n",
    "    return shuffleSplit.split(x,eventVars)\n",
    "\n",
    "def disk_graph_load(batch):\n",
    "    return [torch.load(directory + '/' + graph + '.g') for graph in batch]\n",
    "\n",
    "def get_predictions(model,graphs,features = 'BRCA-CC',device=torch.device('cuda:0')) -> list:\n",
    "    outputs = []\n",
    "    e_and_t = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(graphs)):\n",
    "            graph = graphs[i]\n",
    "            if features == 'BRCA-SHUFFLE':\n",
    "                tag = [graph[0]]\n",
    "                temp = [graph[1][0], graph[1][1]]\n",
    "                graph = disk_graph_load(tag)\n",
    "            else:\n",
    "                temp = [graph.event.item(),graph.e_time.item()]\n",
    "                graph = [graph]\n",
    "            size = 1\n",
    "            loader = DataLoader(graph, batch_size=size)\n",
    "            for d in loader:\n",
    "                d = d.to(device)\n",
    "            z,_,_ = model(d)\n",
    "            z = toNumpy(z)\n",
    "            outputs.append(z[0][0])\n",
    "            e_and_t.append(temp)\n",
    "    return outputs, e_and_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, dim_features, dim_target, layers=[16,16,8],pooling='max',dropout = 0.0,conv='GINConv',gembed=False,**kwargs) -> None:\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        dim_features : TYPE Int\n",
    "            DESCRIPTION. Number of features of each node\n",
    "        dim_target : TYPE Int\n",
    "            DESCRIPTION. Number of outputs\n",
    "        layers : TYPE, optional List of number of nodes in each layer\n",
    "            DESCRIPTION. The default is [6,6].\n",
    "        pooling : TYPE, optional\n",
    "            DESCRIPTION. The default is 'max'.\n",
    "        dropout : TYPE, optional\n",
    "            DESCRIPTION. The default is 0.0.\n",
    "        conv : TYPE, optional Layer type string {'GINConv','EdgeConv'} supported\n",
    "            DESCRIPTION. The default is 'GINConv'.\n",
    "        gembed : TYPE, optional Graph Embedding\n",
    "            DESCRIPTION. The default is False. Pool node scores or pool node features\n",
    "        **kwargs : TYPE\n",
    "            DESCRIPTION.\n",
    "        Raises\n",
    "        ------\n",
    "        NotImplementedError\n",
    "            DESCRIPTION.\n",
    "        Returns\n",
    "        -------\n",
    "        None.\n",
    "        \"\"\"\n",
    "        super(GNN, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.embeddings_dim=layers\n",
    "        self.no_layers = len(self.embeddings_dim)\n",
    "        self.first_h = []\n",
    "        self.nns = []\n",
    "        self.convs = []\n",
    "        self.linears = []\n",
    "        self.pooling = {'max':global_max_pool,'mean':global_mean_pool,'add':global_add_pool}[pooling]\n",
    "        self.gembed = gembed #if True then learn graph embedding for final classification (classify pooled node features) otherwise pool node decision scores\n",
    "\n",
    "        for layer, out_emb_dim in enumerate(self.embeddings_dim):\n",
    "            if layer == 0:\n",
    "                self.first_h = Sequential(Linear(dim_features, out_emb_dim), BatchNorm1d(out_emb_dim),GELU())\n",
    "                self.linears.append(Sequential(Linear(out_emb_dim, dim_target),GELU()))\n",
    "                \n",
    "            else:\n",
    "                input_emb_dim = self.embeddings_dim[layer-1]\n",
    "                self.linears.append(Linear(out_emb_dim, dim_target))\n",
    "                subnet = Sequential(Linear(input_emb_dim, out_emb_dim), BatchNorm1d(out_emb_dim))              \n",
    "                if conv=='GINConv':\n",
    "                    self.nns.append(subnet)\n",
    "                    self.convs.append(GINConv(self.nns[-1], **kwargs))  # Eq. 4.2 eps=100, train_eps=False\n",
    "                elif conv=='EdgeConv':\n",
    "                    subnet = Sequential(Linear(2*input_emb_dim, out_emb_dim), BatchNorm1d(out_emb_dim))\n",
    "                    self.nns.append(subnet)                    \n",
    "                    self.convs.append(EdgeConv(self.nns[-1],**kwargs))#DynamicEdgeConv#EdgeConv                aggr='mean'\n",
    "                else:\n",
    "                    raise NotImplementedError  \n",
    "                    \n",
    "        self.nns = torch.nn.ModuleList(self.nns)\n",
    "        self.convs = torch.nn.ModuleList(self.convs)\n",
    "        self.linears = torch.nn.ModuleList(self.linears)  # has got one more for initial input\n",
    "        \n",
    "    def forward(self, data) -> torch.tensor:\n",
    "\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        \n",
    "        out = 0\n",
    "        pooling = self.pooling\n",
    "        Z = 0\n",
    "        import torch.nn.functional as F\n",
    "        for layer in range(self.no_layers):            \n",
    "            if layer == 0:\n",
    "                x = self.first_h(x)\n",
    "                z = self.linears[layer](x)\n",
    "                Z+=z\n",
    "                dout = F.dropout(pooling(z, batch), p=self.dropout, training=self.training)\n",
    "                out += dout\n",
    "            else:\n",
    "                x = self.convs[layer-1](x,edge_index)\n",
    "                if not self.gembed:\n",
    "                    z = self.linears[layer](x)\n",
    "                    Z+=z\n",
    "                    dout = F.dropout(pooling(z, batch), p=self.dropout, training=self.training)\n",
    "                else:\n",
    "                    dout = F.dropout(self.linears[layer](pooling(x, batch)), p=self.dropout, training=self.training)\n",
    "                out += dout\n",
    "\n",
    "        return out,Z,x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetWrapper:\n",
    "    def __init__(self, model, device='cuda:0',features='BRCA-CC') -> None:\n",
    "        self.model = model\n",
    "        self.device = torch.device(device)\n",
    "        self.features = features\n",
    "\n",
    "    def loss_fn(self,batch,optimizer) -> float:\n",
    "        z = toTensorGPU(0)\n",
    "        loss = 0\n",
    "        unzipped = [j for pair in batch for j in pair]\n",
    "        # This can be changed when using a system with large RAM\n",
    "        if self.features == 'BRCA-SHUFFLE':\n",
    "            graph_set = list(set(unzipped))\n",
    "            graphs = disk_graph_load(graph_set)\n",
    "            unzipped = None\n",
    "        else:\n",
    "            graph_set = unzipped\n",
    "            graphs = graph_set\n",
    "            unzipped = None\n",
    "        batch_load = DataLoader(graphs, batch_size = len(graphs))\n",
    "        for d in batch_load: \n",
    "            d = d.to(self.device)\n",
    "        self.model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output,_,_ = self.model(d)\n",
    "        num_pairs = len(batch)\n",
    "        for (xi,xj) in batch:\n",
    "            graph_i, graph_j = graph_set.index(xi), graph_set.index(xj)\n",
    "            # Compute loss function\n",
    "            dz = output[graph_i] - output[graph_j]\n",
    "            loss += torch.max(z, 1.0 - dz)\n",
    "        loss = loss/num_pairs\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def validation_loss_and_Cindex_eval(self,graphs,pairs) -> float:\n",
    "        tot_loss = 0\n",
    "        print('Number of Validation Pairs: ' + str(len(pairs)))\n",
    "        predictions, e_and_t = get_predictions(self.model,graphs,self.features)\n",
    "        for j in range(len(pairs)):\n",
    "            p_graph_i = predictions[pairs[j][0]]\n",
    "            p_graph_j = predictions[pairs[j][1]]\n",
    "            dz = p_graph_i - p_graph_j\n",
    "            loss = max(0, 1.0 - dz)\n",
    "            tot_loss += loss\n",
    "        epoch_val_loss = tot_loss / len(pairs)\n",
    "        T = [x[1] for x in e_and_t]\n",
    "        E = [x[0] for x in e_and_t]\n",
    "        concord = cindex(T,predictions,E)\n",
    "        return epoch_val_loss, concord\n",
    "\n",
    "    def censor_data(self,graphs, censor_time): # The censor time measured in years\n",
    "        cen_time = 365 * censor_time\n",
    "        for graph in graphs:\n",
    "            if self.features == 'BRCA-SHUFFLE':\n",
    "                time = graph[1][1]\n",
    "            else:\n",
    "                time = graph.e_time\n",
    "            if time > cen_time:\n",
    "                if self.features == 'BRCA-SHUFFLE':\n",
    "                    graph[1] = (0,cen_time)\n",
    "                else:\n",
    "                    graph.event = toTensor(0)\n",
    "                    graph.e_time = toTensor(cen_time)\n",
    "            else:\n",
    "                continue\n",
    "        return graphs\n",
    "\n",
    "    def train(self,training_data,validation_data,max_batches=500,num_logs=50,optimizer=torch.optim.Adam,\n",
    "              early_stopping = 10, return_best = False, batch_size = 10) -> float:\n",
    "            return_best = return_best and validation_data is not None\n",
    "            counter = 0 # To resolve list index errors with large NUM_BATCHES vals\n",
    "            log_interval = max_batches // num_logs\n",
    "            loss_vals = {}\n",
    "            loss_vals['train'] = []\n",
    "            loss_vals['validation'] = []\n",
    "            concords = []\n",
    "            c_best = 0.5\n",
    "            best_batch = 1000\n",
    "            patience = early_stopping\n",
    "            training_indexes = pair_find(training_data,self.features)\n",
    "            print(\"Number of batches used for training \"+ str(max_batches))\n",
    "            print('Num Pairs: ' + str(len(training_indexes)))\n",
    "            best_model = deepcopy(self.model)\n",
    "            for i in tqdm(range(1,max_batches + 1)):\n",
    "                if counter < len(training_indexes) - batch_size:\n",
    "                    batch_pairs = []\n",
    "                    index_pairs = training_indexes[counter:counter+batch_size]\n",
    "                    for j in range(len(index_pairs)):\n",
    "                        if self.features == 'BRCA-SHUFFLE':\n",
    "                            graph_i = training_data[index_pairs[j][0]][0]\n",
    "                            graph_j = training_data[index_pairs[j][1]][0]\n",
    "                        else:\n",
    "                            graph_i = training_data[index_pairs[j][0]]\n",
    "                            graph_j = training_data[index_pairs[j][1]]\n",
    "                        batch_pairs.append((graph_i,graph_j))\n",
    "                    loss = self.loss_fn(batch_pairs,optimizer)\n",
    "                    counter += batch_size\n",
    "                else:\n",
    "                    counter = 0\n",
    "                loss_vals['train'].append(loss)\n",
    "                if i % log_interval == 0:\n",
    "                    if validation_data is not None:\n",
    "                        val_loss, c_val = self.validation_loss_and_Cindex_eval(validation_data,validation_indexes)\n",
    "                        loss_vals['validation'].append(val_loss)\n",
    "                        concords.append(c_val)\n",
    "                        print(\"Current Vali Loss Val: \" + str(val_loss) + \"\\n\")\n",
    "                        print(\"\\n\" + \"Current Loss Val: \" + str(loss) + \"\\n\")\n",
    "                        if return_best and c_val > c_best:\n",
    "                            c_best = c_val\n",
    "                            #best_model = deepcopy(model)\n",
    "                            best_batch = i\n",
    "                        if i - best_batch > patience*log_interval:\n",
    "                            print(\"Early Stopping\")\n",
    "                            #break\n",
    "            return loss_vals, concords, self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def __init__(self, model, device='cuda:0',features = 'BRCA-CC') -> None:\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.features = features\n",
    "\n",
    "    def get_predictions(self,model,graphs,device=torch.device('cuda:0')) -> list:\n",
    "        outputs = []\n",
    "        e_and_t = []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i in range(len(graphs)):\n",
    "                graph = graphs[i]\n",
    "                if self.features == 'BRCA-SHUFFLE':\n",
    "                    tag = [graph[0]]\n",
    "                    temp = [graph[1], graph[1]]\n",
    "                    graph = disk_graph_load(tag)\n",
    "                else:\n",
    "                    temp = [graph.event.item(),graph.e_time.item()]\n",
    "                    graph = [graph]\n",
    "                size = 1\n",
    "                loader = DataLoader(graph, batch_size=size)\n",
    "                for d in loader:\n",
    "                    d = d.to(device)\n",
    "                z,_,_ = model(d)\n",
    "                z = toNumpy(z)\n",
    "                outputs.append(z[0])\n",
    "                e_and_t.append(temp)\n",
    "        return outputs, e_and_t\n",
    "    \n",
    "    def test_evaluation(self,testDataset):\n",
    "        predictions, e_and_t = get_predictions(self.model,testDataset,self.features)\n",
    "        T = [x[1] for x in e_and_t]\n",
    "        E = [x[0] for x in e_and_t]\n",
    "        concord = cindex(T,predictions,E)\n",
    "        return concord\n",
    "    \n",
    "    def K_M_Curves(self, graphs, split_val, mode = 'Train') -> None:\n",
    "        outputs, e_and_t = get_predictions(self.model,graphs,self.features)\n",
    "        T = [x[1] for x in e_and_t]\n",
    "        E = [x[0] for x in e_and_t]\n",
    "        mid = np.median(outputs)\n",
    "        if mode != 'Train':\n",
    "            if split_val > 0:\n",
    "                mid = split_val\n",
    "        else:\n",
    "            print(mid)\n",
    "        T_high = []\n",
    "        T_low = []\n",
    "        E_high = [] \n",
    "        E_low = []\n",
    "        for i in range(len(outputs)):\n",
    "          if outputs[i] <= mid:\n",
    "            T_high.append(T[i])\n",
    "            E_high.append(E[i])\n",
    "          else:\n",
    "            T_low.append(T[i])\n",
    "            E_low.append(E[i])\n",
    "        km_high = KaplanMeierFitter()\n",
    "        km_low = KaplanMeierFitter()\n",
    "        ax = plt.subplot(111)\n",
    "        ax = km_high.fit(T_high, event_observed=E_high, label = 'High').plot_survival_function(ax=ax)\n",
    "        ax = km_low.fit(T_low, event_observed=E_low, label = 'Low').plot_survival_function(ax=ax)\n",
    "        from lifelines.plotting import add_at_risk_counts\n",
    "        add_at_risk_counts(km_high, km_low, ax=ax)\n",
    "        plt.title('Kaplan-Meier estimate')\n",
    "        plt.ylabel('Survival probability')\n",
    "        plt.show()\n",
    "        plt.tight_layout()\n",
    "        from lifelines.statistics import logrank_test\n",
    "        results = logrank_test(T_low, T_high, E_low, E_high)\n",
    "        print(\"p-value %s; log-rank %s\" % (results.p_value, np.round(results.test_statistic, 6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    device = {True:'cuda:0',False:'cpu'}[USE_CUDA] \n",
    "    import pandas as pd\n",
    "    import os\n",
    "    from natsort import natsorted\n",
    "    # This is set up to run on colab vvv\n",
    "    survival_file = r'E:\\graph_surv\\NIHMS978596-supplement-1.xlsx'\n",
    "    cols2read = [VARIABLES,TIME_VAR]\n",
    "    TS = pd.read_excel(survival_file).rename(columns= {'bcr_patient_barcode':'ID'}).set_index('ID')  # path to clinical file\n",
    "    TS = TS[cols2read][TS.type == 'BRCA']\n",
    "    print(TS.shape)\n",
    "    if SHUFFLE_NET:\n",
    "        bdir = r'E:\\graph_surv\\ShuffleNet_0.8_dth_4K/'\n",
    "        # Set up directory for on disk dataset\n",
    "        directory = r'E:\\graph_surv\\Graphs'\n",
    "        try:\n",
    "            os.mkdir(directory)\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "    Exid = 'Slide_Graph CC_feats'\n",
    "    graphlist = glob(os.path.join(bdir, \"*.pkl\"))#[0:100]\n",
    "    print(len(graphlist))\n",
    "    device = 'cuda:0'\n",
    "    cpu = torch.device('cpu')\n",
    "\n",
    "    if FILTER_TRIPLE:\n",
    "        filter_file = 'drive/MyDrive/SlideGraph/TCGA-BRCA-DX_CLINI (8).xlsx'\n",
    "        cols = ['ERStatus','PRStatus','HER2FinalStatus']\n",
    "        db = pd.read_excel(filter_file).rename(columns= {'CompleteTCGAID':'ID'}).set_index('ID')\n",
    "        db = db[cols]\n",
    "\n",
    "    try:\n",
    "        os.mkdir(MODEL_PATH)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    graphlist = natsorted(graphlist)\n",
    "    dataset = []\n",
    "    from tqdm import tqdm\n",
    "    for graph in tqdm(graphlist):\n",
    "        TAG = os.path.split(graph)[-1].split('_')[0][:12]\n",
    "        status = TS.loc[TAG,:][1]\n",
    "        event, event_time = TS.loc[TAG,:][0], TS.loc[TAG,:][1]\n",
    "        if np.isnan(event):\n",
    "            continue\n",
    "        if SHUFFLE_NET:\n",
    "            G = torch.load(graph, map_location='cpu') \n",
    "            # Google Colab may sometimes produce \"Transport endpoint is not connected\" error here\n",
    "            # This is not a bug in the code. Rerunning the cell will fix this.\n",
    "            G = Data(**G.__dict__)\n",
    "        else:\n",
    "            if USE_CUDA:\n",
    "                G = pickleLoad(graph)\n",
    "                G.to('cpu')\n",
    "            else:\n",
    "                G = torch.load(graph, map_location=device)\n",
    "        try:\n",
    "            G.y = toTensorGPU([int(status)], dtype=torch.long, requires_grad = False)\n",
    "        except ValueError:\n",
    "            continue\n",
    "        W = radius_neighbors_graph(toNumpy(G.coords), 1500, mode=\"connectivity\",include_self=False).toarray()\n",
    "        g = toGeometricWW(toNumpy(G.x),W,toNumpy(G.y))\n",
    "        g.coords = G.coords\n",
    "        g.event = toTensor(event)\n",
    "        g.e_time = toTensor(event_time)\n",
    "        if SHUFFLE_NET:\n",
    "            dataset.append([TAG,(event,event_time)])\n",
    "            torch.save(g,directory+'/'+TAG+'.g')\n",
    "        else:\n",
    "            dataset.append(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainingDataset = dataset\n",
    "event_vector = np.array([int(g[1][0]) for g in trainingDataset])\n",
    "\n",
    "folds = 5\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "if SHUFFLE_NET:\n",
    "    G = torch.load(directory + '/TCGA-3C-AALI.g')\n",
    "else:\n",
    "    G = dataset[0]\n",
    "\n",
    "converg_vals = []\n",
    "fold_concord = []\n",
    "eval_metrics = []\n",
    "\n",
    "for train_index, vali_index in SplitBrcaData(trainingDataset,folds,SHUFFLE_NET,0.2):\n",
    "    # get indices for training and testing\n",
    "    \n",
    "    # Set up model and optimizer\n",
    "    model = GNN(dim_features=G.x.shape[1], dim_target = 1, layers = [16,16,8,8],\n",
    "                dropout = 0.0, pooling = 'mean', conv='EdgeConv', aggr = 'max')\n",
    "    net = NetWrapper(model,device = device,features = 'BRCA-SHUFFLE')\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(),\n",
    "                            lr=LEARNING_RATE,\n",
    "                            weight_decay=WEIGHT_DECAY)\n",
    "    x_train = [trainingDataset[i] for i in train_index]\n",
    "    testDataset = [trainingDataset[i] for i in vali_index]\n",
    "    # Only censoring the test data\n",
    "    # x_val = net.censor_data(x_val,10) \n",
    "    losses, concords, BestModel = net.train(x_train,\n",
    "                                              None,\n",
    "                                              optimizer = optimizer,\n",
    "                                              return_best = True,\n",
    "                                              max_batches = NUM_BATCHES)\n",
    "    # Evaluate\n",
    "    testDataset = net.censor_data(testDataset,10)\n",
    "    eval = Evaluator(BestModel,features='BRCA-SHUFFLE')\n",
    "    concord = eval.test_evaluation(testDataset)\n",
    "    print(concord)\n",
    "    eval_metrics.append(concord)\n",
    "    converg_vals.append(losses)\n",
    "    fold_concord.append(concords)\n",
    "    #m = max(concords)\n",
    "\n",
    "avg_c = mean(eval_metrics)\n",
    "stdev_c = stdev(eval_metrics)\n",
    "print(\"Performance on test data over %d folds: \\n\" % folds)\n",
    "print(str(avg_c)+' +/- '+str(stdev_c))\n",
    "print(f\"perf on each split was: {eval_metrics}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
